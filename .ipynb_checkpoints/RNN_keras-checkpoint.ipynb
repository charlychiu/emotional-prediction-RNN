{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zijy6kA-Ej1"
   },
   "source": [
    "# sentiment analysis(opinion mining) on IMDb(Internet Movie Database) comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Global setting\n",
    "LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "\n",
    "# Constants\n",
    "IMDB_MLP_MODEL_NAME = 'imdb_mlp.model'\n",
    "IMDB_MLP_MODEL_WEIG = 'imdb_mlp.h5'\n",
    "\n",
    "logging.basicConfig(format=LOG_FORMAT)\n",
    "logger = logging.getLogger('IMDBb')\n",
    "logger.setLevel(logging.INFO)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wKI-Kgg7CWjd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz...\n",
      "download: ('datas/aclImdb_v1.tar.gz', <http.client.HTTPMessage object at 0x000001A07A9D4518>)\n",
      "Extracting datas/aclImdb_v1.tar.gz to datas...\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"  \n",
    "# 84.1 mb num:50,000, train/test 25,000\n",
    "# path = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "filepath = 'datas/aclImdb_v1.tar.gz'\n",
    "dataPath = 'datas/aclImdb'\n",
    "\n",
    "if not os.path.isfile(filepath):\n",
    "    print('Downloading from {}...'.format(url))\n",
    "    result = urlretrieve(url, filepath)\n",
    "    print('download: {}'.format(result))\n",
    "\n",
    "if not os.path.isdir(dataPath):\n",
    "    print('Extracting {} to datas...'.format(filepath))\n",
    "    tfile = tarfile.open(filepath, 'r:gz')\n",
    "    result = tfile.extractall('datas/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "├── README\n",
    "├── imdb.vocab\n",
    "├── imdbEr.txt\n",
    "├── test\n",
    "│   ├── labeledBow.feat\n",
    "│   ├── neg\n",
    "│   ├── pos\n",
    "│   ├── urls_neg.txt\n",
    "│   └── urls_pos.txt\n",
    "└── train\n",
    "    ├── labeledBow.feat\n",
    "    ├── neg\n",
    "    ├── pos\n",
    "    ├── unsup\n",
    "    ├── unsupBow.feat\n",
    "    ├── urls_neg.txt\n",
    "    ├── urls_pos.txt\n",
    "    └── urls_unsup.txt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMnJ7XQGmQgq"
   },
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_Y1e5vbHmUWN",
    "outputId": "0a96d0e2-1583-4d0b-f617-d424ecc5c573"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charlychiu\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5Fi2xt_mlyH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-16 23:42:14,455 - IMDBb - INFO - Reading training data...\n",
      "2018-09-16 23:43:42,961 - IMDBb - INFO - Reading testing data...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_tag(text):\n",
    "    # Remove HTML markers\n",
    "    re_tag = re.compile(r'<[^>]+>')\n",
    "    return re_tag.sub('', text)\n",
    "\n",
    "\n",
    "'''\n",
    "    Read data from IMDb folders\n",
    "\n",
    "    @param filetype(str):\n",
    "        \"train\" or \"test\"\n",
    "\n",
    "    @return:\n",
    "        Tuple(List of labels, List of articles)\n",
    "'''\n",
    "def read_files(filetype):\n",
    "    file_list = []\n",
    "    positive_path = os.path.join(os.path.join(dataPath, filetype), 'pos')\n",
    "    for f in os.listdir(positive_path):\n",
    "        file_list.append(os.path.join(positive_path, f))\n",
    "\n",
    "    negative_path = os.path.join(os.path.join(dataPath, filetype), 'neg')\n",
    "    for f in os.listdir(negative_path):\n",
    "        file_list.append(os.path.join(negative_path, f))\n",
    "\n",
    "    logger.debug('Read {} with {} files...'.format(filetype, len(file_list)))\n",
    "    all_labels = ([1] * 12500 + [0] * 12500)\n",
    "    all_texts = []\n",
    "    for fi in file_list:\n",
    "        logger.debug('Read {}...'.format(fi))\n",
    "        with open(fi, encoding='utf8') as fh:\n",
    "            all_texts += [remove_tag(\" \".join(fh.readlines()))]\n",
    "\n",
    "    return all_labels, all_texts\n",
    "\n",
    "logger.info('Reading training data...')\n",
    "train_labels, train_text = read_files('train')\n",
    "logger.info('Reading testing data...')\n",
    "test_labels, test_text = read_files('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "9KmezERKUiF3",
    "outputId": "981238bf-97d0-454f-a58c-648f274b6b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:25000 \n",
      "\n",
      "testing data size:25000 \n",
      "\n",
      "feature :I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge. \n",
      "\n",
      "label :1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check data \n",
    "print(\"training data size:%d \\n\" % (len(train_text)))\n",
    "print(\"testing data size:%d \\n\" % (len(test_text)))\n",
    "\n",
    "print(\"feature :%s \\n\" % (test_text[0]))\n",
    "print(\"label :%s \\n\" % (test_labels[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-q7b-_T3Vdwh"
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCXVYdzLKNrT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-16 23:44:31,001 - IMDBb - INFO - Tokenizing document...\n",
      "2018-09-16 23:44:35,570 - IMDBb - INFO - Total 25000 document being handled...\n",
      "2018-09-16 23:44:35,571 - IMDBb - INFO - Top 10 word index:\n",
      "2018-09-16 23:44:35,572 - IMDBb - INFO - Translating raw text into token number list...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t'the'\t1\n",
      "\t'and'\t2\n",
      "\t'a'\t3\n",
      "\t'of'\t4\n",
      "\t'to'\t5\n",
      "\t'is'\t6\n",
      "\t'in'\t7\n",
      "\t'it'\t8\n",
      "\t'i'\t9\n",
      "\t'this'\t10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-16 23:44:42,424 - IMDBb - INFO - Padding/Trimming the token number list to length=100...\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN_OF_TOKEN = 100\n",
    "\n",
    "logger.info('Tokenizing document...')\n",
    "\n",
    "# Create a dictionary of 2,000 words\n",
    "token = Tokenizer(num_words=2000)\n",
    "# Read in all training text and select top 2,000 words according to frequency sorting descendingly\n",
    "token.fit_on_texts(train_text)\n",
    "\n",
    "logger.info('Total {} document being handled...'.format(token.document_count))\n",
    "logger.info('Top 10 word index:')\n",
    "c = 0\n",
    "for t, i in token.word_index.items():\n",
    "    print(\"\\t'{}'\\t{}\".format(t, i))\n",
    "    c += 1\n",
    "    if c == 10:\n",
    "        break\n",
    "print(\"\")\n",
    "logger.info('Translating raw text into token number list...')\n",
    "# convert text to vector\n",
    "x_train_seq = token.texts_to_sequences(train_text)\n",
    "x_test_seq = token.texts_to_sequences(test_text)\n",
    "\n",
    "logger.info('Padding/Trimming the token number list to length={}...'.format(MAX_LEN_OF_TOKEN))\n",
    "# padding \n",
    "x_train = sequence.pad_sequences(x_train_seq, maxlen=MAX_LEN_OF_TOKEN)\n",
    "x_test = sequence.pad_sequences(x_test_seq, maxlen=MAX_LEN_OF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "-q_M0SuETlW3",
    "outputId": "172a4986-044e-4c8c-f1f5-95f484906e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "\n",
      "\n",
      "[308, 6, 3, 1068, 208, 8, 29, 1, 168, 54, 13, 45, 81, 40, 391, 109, 137, 13, 57, 149, 7, 1, 481, 68, 5, 260, 11, 6, 72, 5, 631, 70, 6, 1, 5, 1, 1530, 33, 66, 63, 204, 139, 64, 1229, 1, 4, 1, 222, 899, 28, 68, 4, 1, 9, 693, 2, 64, 1530, 50, 9, 215, 1, 386, 7, 59, 3, 1470, 798, 5, 176, 1, 391, 9, 1235, 29, 308, 3, 352, 343, 142, 129, 5, 27, 4, 125, 1470, 5, 308, 9, 532, 11, 107, 1466, 4, 57, 554, 100, 11, 308, 6, 226, 47, 3, 11, 8, 214]\n",
      "\n",
      "\n",
      "before length: 106\n",
      "before sequence: [308, 6, 3, 1068, 208, 8, 29, 1, 168, 54, 13, 45, 81, 40, 391, 109, 137, 13, 57, 149, 7, 1, 481, 68, 5, 260, 11, 6, 72, 5, 631, 70, 6, 1, 5, 1, 1530, 33, 66, 63, 204, 139, 64, 1229, 1, 4, 1, 222, 899, 28, 68, 4, 1, 9, 693, 2, 64, 1530, 50, 9, 215, 1, 386, 7, 59, 3, 1470, 798, 5, 176, 1, 391, 9, 1235, 29, 308, 3, 352, 343, 142, 129, 5, 27, 4, 125, 1470, 5, 308, 9, 532, 11, 107, 1466, 4, 57, 554, 100, 11, 308, 6, 226, 47, 3, 11, 8, 214]\n",
      "\n",
      "\n",
      "after length: 100\n",
      "after sequence: [  29    1  168   54   13   45   81   40  391  109  137   13   57  149\n",
      "    7    1  481   68    5  260   11    6   72    5  631   70    6    1\n",
      "    5    1 1530   33   66   63  204  139   64 1229    1    4    1  222\n",
      "  899   28   68    4    1    9  693    2   64 1530   50    9  215    1\n",
      "  386    7   59    3 1470  798    5  176    1  391    9 1235   29  308\n",
      "    3  352  343  142  129    5   27    4  125 1470    5  308    9  532\n",
      "   11  107 1466    4   57  554  100   11  308    6  226   47    3   11\n",
      "    8  214]\n"
     ]
    }
   ],
   "source": [
    "# compare the text and vector\n",
    "print(train_text[0])\n",
    "print('\\n')\n",
    "print(x_train_seq[0])\n",
    "print('\\n')\n",
    "\n",
    "# show padding result\n",
    "print(\"before length: %d\" % (len(x_train_seq[0])))\n",
    "print(\"before sequence: %s\" % (x_train_seq[0]))\n",
    "print(\"\\n\")\n",
    "print(\"after length: %d\" % (len(x_train[0])))\n",
    "print(\"after sequence: %s\" % (x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGCN6LoSZYAO"
   },
   "source": [
    "## build pure RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wb0pL8HwZsUm"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQCziqm_cUkF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-16 23:44:45,275 - IMDBb - INFO - Model summary:\n",
      "None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 32)           64000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               819456    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 883,713\n",
      "Trainable params: 883,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = 'mlp'\n",
    "IS_RELOAD = False\n",
    "if MODEL_TYPE == 'mlp':\n",
    "    if os.path.isfile(IMDB_MLP_MODEL_NAME):\n",
    "        # Reload model\n",
    "        logger.debug('Reloading model from {}...'.format(IMDB_MLP_MODEL_NAME))\n",
    "        IS_RELOAD = True\n",
    "        with open(IMDB_MLP_MODEL_NAME, 'r') as f:\n",
    "            loaded_model_json = f.read()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        model.load_weights(IMDB_MLP_MODEL_WEIG)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(output_dim=32,\n",
    "                            input_dim=2000,\n",
    "                            input_length=100))\n",
    "        model.add(Dropout(0.2))\n",
    "        '''Drop 20% neuron during training '''\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=256, activation='relu'))\n",
    "        ''' Total 256 neuron in hidden layers'''\n",
    "        model.add(Dropout(0.35))\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "        ''' Define output layer with 'sigmoid activation' '''\n",
    "\n",
    "logger.info('Model summary:\\n{}\\n'.format(model.summary()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7hfCUhSFs2f"
   },
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "lih63o8wA6gv",
    "outputId": "d30b022e-c5e2-49d9-8209-79bfab76a115"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-16 23:44:45,283 - IMDBb - INFO - Start training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      " - 22s - loss: 0.4783 - acc: 0.7592 - val_loss: 0.5382 - val_acc: 0.7432\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.2662 - acc: 0.8902 - val_loss: 0.5053 - val_acc: 0.7796\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1545 - acc: 0.9456 - val_loss: 0.6859 - val_acc: 0.7410\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.0818 - acc: 0.9724 - val_loss: 0.8090 - val_acc: 0.7594\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0484 - acc: 0.9835 - val_loss: 1.0459 - val_acc: 0.7334\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0379 - acc: 0.9861 - val_loss: 0.9912 - val_acc: 0.7714\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0298 - acc: 0.9895 - val_loss: 1.1950 - val_acc: 0.7488\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0262 - acc: 0.9901 - val_loss: 1.1282 - val_acc: 0.7694\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0302 - acc: 0.9886 - val_loss: 1.2108 - val_acc: 0.7614\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0238 - acc: 0.9914 - val_loss: 1.0297 - val_acc: 0.7950\n",
      "\n",
      "\t[Info] Serialized Keras model to imdb_mlp.model...\n"
     ]
    }
   ],
   "source": [
    "if not IS_RELOAD:\n",
    "    logger.info('Start training process...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    train_history = model.fit(x_train, train_labels, batch_size=100, epochs=10, verbose=2, validation_split=0.2)\n",
    "    print(\"\")\n",
    "    # Serialized model\n",
    "    print(\"\\t[Info] Serialized Keras model to %s...\" % (IMDB_MLP_MODEL_NAME))\n",
    "    with open(IMDB_MLP_MODEL_NAME, 'w') as f:\n",
    "        f.write(model.to_json())\n",
    "    model.save_weights(IMDB_MLP_MODEL_WEIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2K6L_yqlF9vA"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-16 23:45:19,139 - IMDBb - INFO - Start evaluation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 49us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-16 23:45:20,369 - IMDBb - INFO - Score=0.81884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-16 23:45:21,238 - IMDBb - INFO - Show prediction on 2'th test data:\n",
      "2018-09-16 23:45:21,239 - IMDBb - INFO - Ground truth: Pos; prediction result: Pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Start evaluation...')\n",
    "scores = model.evaluate(x_test, test_labels, verbose=1)\n",
    "print(\"\")\n",
    "logger.info('Score={}'.format(scores[1]))\n",
    "\n",
    "predict_classes = model.predict_classes(x_test).reshape(-1)\n",
    "print(\"\")\n",
    "sentiDict = {1: 'Pos', 0: 'Neg'}\n",
    "\n",
    "\n",
    "def display_test_Sentiment(i):\n",
    "    r'''\n",
    "    Show prediction on i'th test data\n",
    "    '''\n",
    "    logger.debug('{}\\'th test data:\\n{}\\n'.format(i, test_text[i]))\n",
    "    logger.info(\n",
    "        'Ground truth: {}; prediction result: {}'.format(sentiDict[test_labels[i]], sentiDict[predict_classes[i]]))\n",
    "\n",
    "\n",
    "logger.info('Show prediction on 2\\'th test data:')\n",
    "display_test_Sentiment(2)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "PpSK-A4MAk7g",
    "mMnJ7XQGmQgq",
    "GGCN6LoSZYAO",
    "Y7hfCUhSFs2f",
    "2K6L_yqlF9vA",
    "GlWMd6msGt0A"
   ],
   "name": "RNN_keras.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
