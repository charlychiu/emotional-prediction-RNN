{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zijy6kA-Ej1",
    "colab_type": "text"
   },
   "source": [
    "# sentiment analysis(opinion mining) on IMDb(Internet Movie Database) comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Global setting\n",
    "LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "\n",
    "# Constants\n",
    "IMDB_MLP_MODEL_NAME = 'imdb_mlp.model'\n",
    "IMDB_MLP_MODEL_WEIG = 'imdb_mlp.h5'\n",
    "\n",
    "logging.basicConfig(format=LOG_FORMAT)\n",
    "logger = logging.getLogger('IMDBb')\n",
    "logger.setLevel(logging.INFO)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKI-Kgg7CWjd",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"  \n",
    "# 84.1 mb num:50,000, train/test 25,000\n",
    "# path = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "filepath = 'datas/aclImdb_v1.tar.gz'\n",
    "dataPath = 'datas/aclImdb'\n",
    "\n",
    "if not os.path.isfile(filepath):\n",
    "    print('Downloading from {}...'.format(url))\n",
    "    result = urlretrieve(url, filepath)\n",
    "    print('download: {}'.format(result))\n",
    "\n",
    "if not os.path.isdir(dataPath):\n",
    "    print('Extracting {} to datas...'.format(filepath))\n",
    "    tfile = tarfile.open(filepath, 'r:gz')\n",
    "    result = tfile.extractall('datas/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "├── README\n",
    "├── imdb.vocab\n",
    "├── imdbEr.txt\n",
    "├── test\n",
    "│   ├── labeledBow.feat\n",
    "│   ├── neg\n",
    "│   ├── pos\n",
    "│   ├── urls_neg.txt\n",
    "│   └── urls_pos.txt\n",
    "└── train\n",
    "    ├── labeledBow.feat\n",
    "    ├── neg\n",
    "    ├── pos\n",
    "    ├── unsup\n",
    "    ├── unsupBow.feat\n",
    "    ├── urls_neg.txt\n",
    "    ├── urls_pos.txt\n",
    "    └── urls_unsup.txt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMnJ7XQGmQgq",
    "colab_type": "text"
   },
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_Y1e5vbHmUWN",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "0a96d0e2-1583-4d0b-f617-d424ecc5c573"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5Fi2xt_mlyH",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_tag(text):\n",
    "    # Remove HTML markers\n",
    "    re_tag = re.compile(r'<[^>]+>')\n",
    "    return re_tag.sub('', text)\n",
    "\n",
    "\n",
    "'''\n",
    "    Read data from IMDb folders\n",
    "\n",
    "    @param filetype(str):\n",
    "        \"train\" or \"test\"\n",
    "\n",
    "    @return:\n",
    "        Tuple(List of labels, List of articles)\n",
    "'''\n",
    "def read_files(filetype):\n",
    "    file_list = []\n",
    "    positive_path = os.path.join(os.path.join(dataPath, filetype), 'pos')\n",
    "    for f in os.listdir(positive_path):\n",
    "        file_list.append(os.path.join(positive_path, f))\n",
    "\n",
    "    negative_path = os.path.join(os.path.join(dataPath, filetype), 'neg')\n",
    "    for f in os.listdir(negative_path):\n",
    "        file_list.append(os.path.join(negative_path, f))\n",
    "\n",
    "    logger.debug('Read {} with {} files...'.format(filetype, len(file_list)))\n",
    "    all_labels = ([1] * 12500 + [0] * 12500)\n",
    "    all_texts = []\n",
    "    for fi in file_list:\n",
    "        logger.debug('Read {}...'.format(fi))\n",
    "        with open(fi, encoding='utf8') as fh:\n",
    "            all_texts += [remove_tag(\" \".join(fh.readlines()))]\n",
    "\n",
    "    return all_labels, all_texts\n",
    "\n",
    "logger.info('Reading training data...')\n",
    "train_labels, train_text = read_files('train')\n",
    "logger.info('Reading testing data...')\n",
    "test_labels, test_text = read_files('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9KmezERKUiF3",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105.0
    },
    "outputId": "981238bf-97d0-454f-a58c-648f274b6b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:25000\ntesting data size:25000\nfeature :Bromwell High is a cartoon comedy It ran at the same time as some other programs about school life such as \"Teachers\" My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\" The scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools I knew and their students When I saw the episode in which a student repeatedly tried to burn down the school I immediately recalled  at  High A classic line: INSPECTOR: I'm here to sack one of your teachers STUDENT: Welcome to Bromwell High I expect that many adults of my age think that Bromwell High is far fetched What a pity that it isn't!\nlabel :1\n"
     ]
    }
   ],
   "source": [
    "# check data \n",
    "print(\"training data size:%d \\n\" % (len(train_text)))\n",
    "print(\"testing data size:%d \\n\" % (len(test_text)))\n",
    "\n",
    "print(\"feature :%s \\n\" % (test_text[0]))\n",
    "print(\"label :%s \\n\" % (test_labels[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q7b-_T3Vdwh",
    "colab_type": "text"
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SCXVYdzLKNrT",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "MAX_LEN_OF_TOKEN = 100\n",
    "\n",
    "logger.info('Tokenizing document...')\n",
    "\n",
    "# Create a dictionary of 2,000 words\n",
    "token = Tokenizer(num_words=2000)\n",
    "# Read in all training text and select top 2,000 words according to frequency sorting descendingly\n",
    "token.fit_on_texts(train_text)\n",
    "\n",
    "logger.info('Total {} document being handled...'.format(token.document_count))\n",
    "logger.info('Top 10 word index:')\n",
    "c = 0\n",
    "for t, i in token.word_index.items():\n",
    "    print(\"\\t'{}'\\t{}\".format(t, i))\n",
    "    c += 1\n",
    "    if c == 10:\n",
    "        break\n",
    "print(\"\")\n",
    "logger.info('Translating raw text into token number list...')\n",
    "# convert text to vector\n",
    "x_train_seq = token.texts_to_sequences(train_text)\n",
    "x_test_seq = token.texts_to_sequences(test_text)\n",
    "\n",
    "logger.info('Padding/Trimming the token number list to length={}...'.format(MAX_LEN_OF_TOKEN))\n",
    "# padding \n",
    "x_train = sequence.pad_sequences(x_train_seq, maxlen=MAX_LEN_OF_TOKEN)\n",
    "x_test = sequence.pad_sequences(x_test_seq, maxlen=MAX_LEN_OF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-q_M0SuETlW3",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258.0
    },
    "outputId": "172a4986-044e-4c8c-f1f5-95f484906e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy It ran at the same time as some other programs about school life such as \"Teachers\" My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\" The scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools I knew and their students When I saw the episode in which a student repeatedly tried to burn down the school I immediately recalled  at  High A classic line: INSPECTOR: I'm here to sack one of your teachers STUDENT: Welcome to Bromwell High I expect that many adults of my age think that Bromwell High is far fetched What a pity that it isn't!\n\n\n[301, 6, 3, 1070, 212, 8, 29, 1, 168, 56, 13, 45, 81, 40, 388, 113, 134, 13, 58, 149, 7, 1, 472, 68, 5, 256, 11, 1984, 6, 72, 5, 636, 70, 6, 1, 5, 1, 1515, 33, 66, 64, 203, 140, 63, 1248, 1, 4, 1, 218, 915, 28, 68, 4, 1, 10, 683, 2, 63, 1515, 51, 10, 209, 1, 391, 7, 59, 3, 1463, 789, 5, 178, 1, 388, 10, 1223, 29, 301, 3, 354, 341, 146, 132, 5, 27, 4, 124, 1463, 5, 301, 10, 525, 11, 106, 1487, 4, 58, 555, 100, 11, 301, 6, 225, 46, 3, 11, 8, 210]\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before length: 107\nbefore sequence: [301, 6, 3, 1070, 212, 8, 29, 1, 168, 56, 13, 45, 81, 40, 388, 113, 134, 13, 58, 149, 7, 1, 472, 68, 5, 256, 11, 1984, 6, 72, 5, 636, 70, 6, 1, 5, 1, 1515, 33, 66, 64, 203, 140, 63, 1248, 1, 4, 1, 218, 915, 28, 68, 4, 1, 10, 683, 2, 63, 1515, 51, 10, 209, 1, 391, 7, 59, 3, 1463, 789, 5, 178, 1, 388, 10, 1223, 29, 301, 3, 354, 341, 146, 132, 5, 27, 4, 124, 1463, 5, 301, 10, 525, 11, 106, 1487, 4, 58, 555, 100, 11, 301, 6, 225, 46, 3, 11, 8, 210]\n\n\nafter length: 100\nafter sequence: [   1  168   56   13   45   81   40  388  113  134   13   58  149    7\n    1  472   68    5  256   11 1984    6   72    5  636   70    6    1\n    5    1 1515   33   66   64  203  140   63 1248    1    4    1  218\n  915   28   68    4    1   10  683    2   63 1515   51   10  209    1\n  391    7   59    3 1463  789    5  178    1  388   10 1223   29  301\n    3  354  341  146  132    5   27    4  124 1463    5  301   10  525\n   11  106 1487    4   58  555  100   11  301    6  225   46    3   11\n    8  210]\n"
     ]
    }
   ],
   "source": [
    "# compare the text and vector\n",
    "print(train_text[0])\n",
    "print('\\n')\n",
    "print(x_train_seq[0])\n",
    "print('\\n')\n",
    "\n",
    "# show padding result\n",
    "print(\"before length: %d\" % (len(x_train_seq[0])))\n",
    "print(\"before sequence: %s\" % (x_train_seq[0]))\n",
    "print(\"\\n\")\n",
    "print(\"after length: %d\" % (len(x_train[0])))\n",
    "print(\"after sequence: %s\" % (x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGCN6LoSZYAO",
    "colab_type": "text"
   },
   "source": [
    "## build pure RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Wb0pL8HwZsUm",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yQCziqm_cUkF",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'mlp'\n",
    "IS_RELOAD = False\n",
    "if MODEL_TYPE == 'mlp':\n",
    "    if os.path.isfile(IMDB_MLP_MODEL_NAME):\n",
    "        # Reload model\n",
    "        logger.debug('Reloading model from {}...'.format(IMDB_MLP_MODEL_NAME))\n",
    "        IS_RELOAD = True\n",
    "        with open(IMDB_MLP_MODEL_NAME, 'r') as f:\n",
    "            loaded_model_json = f.read()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        model.load_weights(IMDB_MLP_MODEL_WEIG)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(output_dim=32,\n",
    "                            input_dim=2000,\n",
    "                            input_length=100))\n",
    "        model.add(Dropout(0.2))\n",
    "        '''Drop 20% neuron during training '''\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=256, activation='relu'))\n",
    "        ''' Total 256 neuron in hidden layers'''\n",
    "        model.add(Dropout(0.35))\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "        ''' Define output layer with 'sigmoid activation' '''\n",
    "\n",
    "logger.info('Model summary:\\n{}\\n'.format(model.summary()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7hfCUhSFs2f",
    "colab_type": "text"
   },
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "lih63o8wA6gv",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374.0
    },
    "outputId": "d30b022e-c5e2-49d9-8209-79bfab76a115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\nEpoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 38s - loss: 0.5487 - acc: 0.7169 - val_loss: 0.4495 - val_acc: 0.8188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 15s - loss: 0.3755 - acc: 0.8394 - val_loss: 0.5606 - val_acc: 0.7502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 15s - loss: 0.3280 - acc: 0.8635 - val_loss: 0.4177 - val_acc: 0.8174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 16s - loss: 0.2867 - acc: 0.8827 - val_loss: 0.4529 - val_acc: 0.8180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 15s - loss: 0.2566 - acc: 0.8958 - val_loss: 0.5137 - val_acc: 0.8096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 15s - loss: 0.2302 - acc: 0.9080 - val_loss: 0.6379 - val_acc: 0.7386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 15s - loss: 0.2027 - acc: 0.9195 - val_loss: 0.8288 - val_acc: 0.7586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 15s - loss: 0.1843 - acc: 0.9279 - val_loss: 0.5798 - val_acc: 0.8348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 15s - loss: 0.1704 - acc: 0.9339 - val_loss: 0.6810 - val_acc: 0.7698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 15s - loss: 0.1478 - acc: 0.9424 - val_loss: 0.8034 - val_acc: 0.7438\n"
     ]
    }
   ],
   "source": [
    "if not IS_RELOAD:\n",
    "    logger.info('Start training process...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    train_history = model.fit(x_train, train_labels, batch_size=100, epochs=10, verbose=2, validation_split=0.2)\n",
    "    print(\"\")\n",
    "    # Serialized model\n",
    "    print(\"\\t[Info] Serialized Keras model to %s...\" % (IMDB_MLP_MODEL_NAME))\n",
    "    with open(IMDB_MLP_MODEL_NAME, 'w') as f:\n",
    "        f.write(model.to_json())\n",
    "    model.save_weights(IMDB_MLP_MODEL_WEIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K6L_yqlF9vA",
    "colab_type": "text"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Start evaluation...')\n",
    "scores = model.evaluate(x_test, test_labels, verbose=1)\n",
    "print(\"\")\n",
    "logger.info('Score={}'.format(scores[1]))\n",
    "\n",
    "predict_classes = model.predict_classes(x_test).reshape(-1)\n",
    "print(\"\")\n",
    "sentiDict = {1: 'Pos', 0: 'Neg'}\n",
    "\n",
    "\n",
    "def display_test_Sentiment(i):\n",
    "    r'''\n",
    "    Show prediction on i'th test data\n",
    "    '''\n",
    "    logger.debug('{}\\'th test data:\\n{}\\n'.format(i, test_text[i]))\n",
    "    logger.info(\n",
    "        'Ground truth: {}; prediction result: {}'.format(sentiDict[test_labels[i]], sentiDict[predict_classes[i]]))\n",
    "\n",
    "\n",
    "logger.info('Show prediction on 2\\'th test data:')\n",
    "display_test_Sentiment(2)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RNN_keras.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [
    "PpSK-A4MAk7g",
    "mMnJ7XQGmQgq",
    "GGCN6LoSZYAO",
    "Y7hfCUhSFs2f",
    "2K6L_yqlF9vA",
    "GlWMd6msGt0A"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
